{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Check if the environment is valid\n",
    "env = TreasureHuntEnv(grid_size=5, max_steps=50)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# Initialize the DQN model\n",
    "#model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_treasure_hunt/\")\n",
    "#model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"/Users/eshajaiswal/Library/CloudStorage/OneDrive-Personal/Esha THM Original/THM 5.Sem/5. KI/DungeonProjectRFL/dqn_treasure_hunt/\")\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"dqn_treasure_hunt\")\n",
    "\n",
    "# Test the trained model\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the TreasureHunt Environment\n",
    "class TreasureHuntEnv(gym.Env):\n",
    "    def __init__(self, grid_size=10, max_steps=100):\n",
    "        super(TreasureHuntEnv, self).__init__()\n",
    "\n",
    "        # Grid Configuration\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Action space: 4 discrete actions (up, down, left, right)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: Grid flattened into a single vector\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=3, shape=(grid_size, grid_size), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        # Rewards\n",
    "        self.reward_treasure = 10\n",
    "        self.reward_trap = -5\n",
    "        self.reward_exit = 50\n",
    "        self.step_penalty = -1\n",
    "\n",
    "        # Initialize the environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset step counter\n",
    "        self.steps = 0\n",
    "\n",
    "        # Create a new grid\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.int32)\n",
    "\n",
    "        # Place treasures (value = 1)\n",
    "        for _ in range(10):\n",
    "            x, y = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Place traps (value = 2)\n",
    "        for _ in range(10):\n",
    "            x, y = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)\n",
    "            if self.grid[x, y] == 0:  # Ensure no overlap\n",
    "                self.grid[x, y] = 2\n",
    "\n",
    "        # Place exit (value = 3)\n",
    "        self.grid[self.grid_size - 1, self.grid_size - 1] = 3\n",
    "\n",
    "        # Player's starting position\n",
    "        self.player_pos = [0, 0]\n",
    "\n",
    "        # Observation: Initial grid state\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        # Move player based on the action\n",
    "        if action == 0 and self.player_pos[0] > 0:  # Up\n",
    "            self.player_pos[0] -= 1\n",
    "        elif action == 1 and self.player_pos[0] < self.grid_size - 1:  # Down\n",
    "            self.player_pos[0] += 1\n",
    "        elif action == 2 and self.player_pos[1] > 0:  # Left\n",
    "            self.player_pos[1] -= 1\n",
    "        elif action == 3 and self.player_pos[1] < self.grid_size - 1:  # Right\n",
    "            self.player_pos[1] += 1\n",
    "\n",
    "        # Calculate reward\n",
    "        current_cell = self.grid[self.player_pos[0], self.player_pos[1]]\n",
    "        reward = self.step_penalty  # Default step penalty\n",
    "\n",
    "        if current_cell == 1:  # Treasure\n",
    "            reward += self.reward_treasure\n",
    "            self.grid[self.player_pos[0], self.player_pos[1]] = 0  # Remove treasure\n",
    "        elif current_cell == 2:  # Trap\n",
    "            reward += self.reward_trap\n",
    "        elif current_cell == 3:  # Exit\n",
    "            reward += self.reward_exit\n",
    "            return self._get_observation(), reward, True, False, {}\n",
    "\n",
    "        # Check termination\n",
    "        done = self.steps >= self.max_steps\n",
    "        return self._get_observation(), reward, done, False, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.copy(self.grid)\n",
    "\n",
    "# Funktion zum Trainieren und Evaluieren des Modells mit den neuen Hyperparametern\n",
    "def train_dqn(env, learning_rate, gamma, batch_size, buffer_size, exploration_initial_eps, \n",
    "              exploration_final_eps, exploration_fraction, target_update_interval, train_freq, total_timesteps):\n",
    "    \n",
    "    model = DQN(\"MlpPolicy\", env, \n",
    "                learning_rate=learning_rate,\n",
    "                gamma=gamma,\n",
    "                batch_size=batch_size,\n",
    "                buffer_size=buffer_size,\n",
    "                exploration_initial_eps=exploration_initial_eps,\n",
    "                exploration_final_eps=exploration_final_eps,\n",
    "                exploration_fraction=exploration_fraction,\n",
    "                target_update_interval=target_update_interval,\n",
    "                train_freq=train_freq,\n",
    "                verbose=1)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    # Modell evaluieren\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "    return model, mean_reward, std_reward\n",
    "\n",
    "# Setzen der verbesserten Hyperparameter f端r den zweiten Trainingsfall\n",
    "updated_hyperparameters = {\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"gamma\": 0.98,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"exploration_initial_eps\": 1.0,\n",
    "    \"exploration_final_eps\": 0.02,\n",
    "    \"exploration_fraction\": 0.99,\n",
    "    \"target_update_interval\": 500,\n",
    "    \"train_freq\": 2,\n",
    "    \"total_timesteps\": 20000\n",
    "}\n",
    "\n",
    "# Umgebung initialisieren\n",
    "env = TreasureHuntEnv(grid_size=5, max_steps=50)\n",
    "\n",
    "# Training mit den verbesserten Hyperparametern ausf端hren\n",
    "model, mean_reward, std_reward = train_dqn(env, **updated_hyperparameters)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(f\"\\nErgebnisse nach Training mit verbesserten Hyperparametern:\\n\"\n",
    "      f\"Durchschnittliche Belohnung: {mean_reward}\\n\"\n",
    "      f\"Standardabweichung der Belohnung: {std_reward}\\n\")\n",
    "\n",
    "# Modell speichern\n",
    "model.save(\"dqn_treasure_hunt_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the TreasureHunt Environment\n",
    "class TreasureHuntEnv(gym.Env):\n",
    "    def __init__(self, grid_size=10, max_steps=100):\n",
    "        super(TreasureHuntEnv, self).__init__()\n",
    "\n",
    "        # Grid Configuration\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Action space: 4 discrete actions (up, down, left, right)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: Grid flattened into a single vector\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=3, shape=(grid_size, grid_size), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        # Rewards\n",
    "        self.reward_treasure = 10\n",
    "        self.reward_trap = -5\n",
    "        self.reward_exit = 50\n",
    "        self.step_penalty = -1\n",
    "\n",
    "        # Initialize the environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset step counter\n",
    "        self.steps = 0\n",
    "\n",
    "        # Create a new grid\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.int32)\n",
    "\n",
    "        # Place treasures (value = 1)\n",
    "        for _ in range(10):\n",
    "            x, y = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Place traps (value = 2)\n",
    "        for _ in range(10):\n",
    "            x, y = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)\n",
    "            if self.grid[x, y] == 0:  # Ensure no overlap\n",
    "                self.grid[x, y] = 2\n",
    "\n",
    "        # Place exit (value = 3)\n",
    "        self.grid[self.grid_size - 1, self.grid_size - 1] = 3\n",
    "\n",
    "        # Player's starting position\n",
    "        self.player_pos = [0, 0]\n",
    "\n",
    "        # Observation: Initial grid state\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        # Move player based on the action\n",
    "        if action == 0 and self.player_pos[0] > 0:  # Up\n",
    "            self.player_pos[0] -= 1\n",
    "        elif action == 1 and self.player_pos[0] < self.grid_size - 1:  # Down\n",
    "            self.player_pos[0] += 1\n",
    "        elif action == 2 and self.player_pos[1] > 0:  # Left\n",
    "            self.player_pos[1] -= 1\n",
    "        elif action == 3 and self.player_pos[1] < self.grid_size - 1:  # Right\n",
    "            self.player_pos[1] += 1\n",
    "\n",
    "        # Calculate reward\n",
    "        current_cell = self.grid[self.player_pos[0], self.player_pos[1]]\n",
    "        reward = self.step_penalty  # Default step penalty\n",
    "\n",
    "        if current_cell == 1:  # Treasure\n",
    "            reward += self.reward_treasure\n",
    "            self.grid[self.player_pos[0], self.player_pos[1]] = 0  # Remove treasure\n",
    "        elif current_cell == 2:  # Trap\n",
    "            reward += self.reward_trap\n",
    "        elif current_cell == 3:  # Exit\n",
    "            reward += self.reward_exit\n",
    "            return self._get_observation(), reward, True, False, {}\n",
    "\n",
    "        # Check termination\n",
    "        done = self.steps >= self.max_steps\n",
    "        return self._get_observation(), reward, done, False, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.copy(self.grid)\n",
    "\n",
    "# Funktion zum Trainieren und Evaluieren des Modells mit den neuen Hyperparametern\n",
    "def train_dqn(env, learning_rate, gamma, batch_size, buffer_size, exploration_initial_eps, \n",
    "              exploration_final_eps, exploration_fraction, target_update_interval, train_freq, total_timesteps):\n",
    "    \n",
    "    model = DQN(\"MlpPolicy\", env, \n",
    "                learning_rate=learning_rate,\n",
    "                gamma=gamma,\n",
    "                batch_size=batch_size,\n",
    "                buffer_size=buffer_size,\n",
    "                exploration_initial_eps=exploration_initial_eps,\n",
    "                exploration_final_eps=exploration_final_eps,\n",
    "                exploration_fraction=exploration_fraction,\n",
    "                target_update_interval=target_update_interval,\n",
    "                train_freq=train_freq,\n",
    "                verbose=1)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    # Modell evaluieren\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "    return model, mean_reward, std_reward\n",
    "\n",
    "# Setzen der optimierten Hyperparameter f端r den dritten Trainingsfall\n",
    "optimized_hyperparameters = {\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"gamma\": 0.97,\n",
    "    \"batch_size\": 256,\n",
    "    \"buffer_size\": 200000,\n",
    "    \"exploration_initial_eps\": 1.0,\n",
    "    \"exploration_final_eps\": 0.005,\n",
    "    \"exploration_fraction\": 0.995,\n",
    "    \"target_update_interval\": 250,\n",
    "    \"train_freq\": 1,\n",
    "    \"total_timesteps\": 50000\n",
    "}\n",
    "\n",
    "# Umgebung initialisieren\n",
    "env = TreasureHuntEnv(grid_size=5, max_steps=50)\n",
    "\n",
    "# Training mit den optimierten Hyperparametern ausf端hren\n",
    "model, mean_reward, std_reward = train_dqn(env, **optimized_hyperparameters)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(f\"\\nErgebnisse nach Training mit optimierten Hyperparametern:\\n\"\n",
    "      f\"Durchschnittliche Belohnung: {mean_reward}\\n\"\n",
    "      f\"Standardabweichung der Belohnung: {std_reward}\\n\")\n",
    "\n",
    "# Modell speichern\n",
    "model.save(\"dqn_treasure_hunt_v3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
